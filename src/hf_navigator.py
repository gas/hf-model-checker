import questionary
from huggingface_hub import HfApi, scan_cache_dir
import subprocess
import sys
import os
import psutil
import torch
import threading
import time

# --- ConfiguraciÃ³n Global ---
api = HfApi()

CATEGORIES = {
    "ğŸ” BÃºsqueda Manual": "search",
    "ğŸ’» ProgramaciÃ³n (Coding)": "coder",
    "ğŸ§  Razonamiento (Reasoning/R1)": "reasoning",
    "ğŸ’¬ Chat General (Instruct)": "instruct",
    "ğŸ‘ï¸ VisiÃ³n / Multimodal": "multimodal",
    "ğŸ­ Roleplay / Historia": "roleplay",
    "ğŸ§ª Modelos PequeÃ±os (<3B)": "smol",
    "ğŸ  Ver mis modelos descargados": "local_only"
}

# Variable global para la cachÃ© (se llena en segundo plano)
LOCAL_CACHE_REPOS = set()
CACHE_READY = False

# --- Funciones de Sistema y Fondo ---

def get_hardware_info():
    """Obtiene info bÃ¡sica rÃ¡pido para la cabecera"""
    ram = psutil.virtual_memory().total / (1024**3)
    vram = 0
    if torch.cuda.is_available():
        vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    return ram, vram

def background_cache_scanner():
    """Escanea la cachÃ© sin bloquear el menÃº"""
    global LOCAL_CACHE_REPOS, CACHE_READY
    try:
        cache_info = scan_cache_dir()
        repos = set()
        for repo in cache_info.repos:
            # Solo nos interesan repos con GGUFs
            has_gguf = any(f.file_name.lower().endswith(".gguf") for r in repo.revisions for f in r.files)
            if has_gguf:
                repos.add(repo.repo_id)
        LOCAL_CACHE_REPOS = repos
        CACHE_READY = True
    except Exception:
        pass

# --- LÃ³gica de BÃºsqueda ---

def get_models(query, limit=10):
    """Busca en la API de HF"""
    print(f"\nğŸŒ Buscando '{query}' (Top {limit})...")
    try:
        models = api.list_models(
            filter="gguf",
            search=query,
            sort="trending_score",
            direction=-1,
            limit=limit
        )
        return [m.modelId for m in models]
    except Exception as e:
        print(f"Error de conexiÃ³n: {e}")
        return []

# --- MenÃº Principal ---

def main():
    # 1. Iniciar escÃ¡ner en hilo separado (AsÃ­ncrono real)
    scanner_thread = threading.Thread(target=background_cache_scanner, daemon=True)
    scanner_thread.start()

    # 2. Leer hardware una sola vez al inicio
    ram_sys, vram_sys = get_hardware_info()
    
    current_limit = 10
    last_query = None
    last_tag = None

    while True:
        os.system('cls' if os.name == 'nt' else 'clear')
        
        # Cabecera Informativa
        print("==========================================")
        print(f"   ğŸš€ HF MODEL CHECKER  |  RAM: {ram_sys:.1f}GB  VRAM: {vram_sys:.1f}GB")
        if CACHE_READY:
            print(f"   ğŸ’¾ CachÃ© Local: {len(LOCAL_CACHE_REPOS)} modelos detectados")
        else:
            print("   ğŸ’¾ CachÃ© Local: Escaneando en segundo plano...")
        print("==========================================\n")

        # SelecciÃ³n de CategorÃ­a
        cat_name = questionary.select(
            "Â¿QuÃ© quieres explorar hoy?",
            choices=list(CATEGORIES.keys()) + ["âŒ Salir"]
        ).ask()

        if cat_name == "âŒ Salir" or cat_name is None:
            break

        tag = CATEGORIES[cat_name]
        
        # GestiÃ³n de bÃºsqueda nueva vs paginaciÃ³n
        if tag != last_tag:
            current_limit = 10 # Reset paginaciÃ³n si cambiamos de categorÃ­a
            last_tag = tag

        model_ids = []

        # LÃ³gica por tipo de categorÃ­a
        if tag == "local_only":
            # Esperar a que la cachÃ© estÃ© lista si el usuario pide ver SOLO lo local
            if not CACHE_READY:
                print("â³ Esperando al escÃ¡ner de disco...")
                while not CACHE_READY: time.sleep(0.5)
            model_ids = list(LOCAL_CACHE_REPOS)
        
        elif tag == "search":
            query = questionary.text("Escribe el nombre (ej: Mistral):").ask()
            if not query: continue
            last_query = query
            model_ids = get_models(query, limit=current_limit)
        
        else:
            # CategorÃ­as predefinidas
            last_query = tag # Para usarlo en la paginaciÃ³n
            model_ids = get_models(tag, limit=current_limit)

        if not model_ids:
            print("âŒ No se encontraron modelos.")
            questionary.press_any_key_to_continue().ask()
            continue

        # --- Sub-MenÃº de Resultados con PaginaciÃ³n ---
        while True:
            choices = []
            for m_id in model_ids:
                # Icono dinÃ¡mico: Si el hilo de fondo ya lo encontrÃ³, pone la casita
                prefix = "ğŸ " if (CACHE_READY and m_id in LOCAL_CACHE_REPOS) else "ğŸŒ"
                urlbase = "https://huggingface.co/"
                choices.append(questionary.Choice(title=f"{prefix} {urlbase}{m_id}", value=m_id))
            
            # Botones de control
            if tag != "local_only": # No paginamos lo local
                choices.append(questionary.Choice(title="â¬‡ï¸  Cargar 10 mÃ¡s...", value="LOAD_MORE"))
            
            choices.append(questionary.Choice(title="â¬…ï¸  Volver al menÃº", value="BACK"))

            selected = questionary.select(
                f"Resultados ({len(model_ids)}):",
                choices=choices
            ).ask()

            if selected == "BACK":
                break
            
            elif selected == "LOAD_MORE":
                current_limit += 10
                # Recargamos usando la query guardada
                if tag == "search":
                    model_ids = get_models(last_query, limit=current_limit)
                else:
                    model_ids = get_models(last_tag, limit=current_limit)
                continue # Volvemos a pintar la lista con los nuevos items
            
            elif selected:
                # Ejecutar el Checker
                
                # 1. Obtener la ruta absoluta donde vive ESTE script (hf_navigator.py)
                script_dir = os.path.dirname(os.path.abspath(__file__))
                
                # 2. Construir la ruta completa al checker
                checker_path = os.path.join(script_dir, "hf_model_checker.py")

                # 3. Ejecutar usando la ruta absoluta
                # Verificamos si existe por si acaso
                if os.path.exists(checker_path):
                    # Pasamos la VRAM como argumento para no recalcularla en el hijo si no queremos
                    subprocess.run([sys.executable, checker_path, "--model", selected])
                else:
                    print(f"âŒ Error: No encuentro {checker_path}")                


                print("\n" + "-"*50)
                questionary.press_any_key_to_continue().ask()
                # Al volver, redibujamos la lista (break del submenu visual, no del loop principal)
                # O podemos hacer continue para quedarnos en la lista. Haremos continue visual.
                os.system('cls' if os.name == 'nt' else 'clear')
                print(f"--- Viendo resultados para: {cat_name} ---")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Saliendo...")